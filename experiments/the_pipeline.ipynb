{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "the_pipeline.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZy5GxNSkR_O",
        "outputId": "2f1abfee-a1d8-4f07-f0c4-e30b528098b0"
      },
      "source": [
        "!pip install ffmpeg-python"
      ],
      "id": "GZy5GxNSkR_O",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ffmpeg-python\n",
            "  Downloading https://files.pythonhosted.org/packages/d7/0c/56be52741f75bad4dc6555991fabd2e07b432d333da82c11ad701123888a/ffmpeg_python-0.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from ffmpeg-python) (0.16.0)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzvVLr8Z7ybV"
      },
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import ffmpeg\n",
        "import PIL\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from tqdm.notebook import tqdm\n",
        "import json"
      ],
      "id": "IzvVLr8Z7ybV",
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93eM-hTMI-KH"
      },
      "source": [
        "from IPython import display\n",
        "from PIL import Image"
      ],
      "id": "93eM-hTMI-KH",
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lvI9o5T6_dB",
        "outputId": "50bf3350-2bfe-4f92-9009-cf7780d69ecc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "PREFIX = \"/content/drive/MyDrive/traffic-light-detector\""
      ],
      "id": "_lvI9o5T6_dB",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YErqPaC968mO"
      },
      "source": [
        "OBJECT_DETECTION_MODEL = f'{PREFIX}/pretrained/yolov5s.pt'\n",
        "CLASSIFICATION_MODEL = f'{PREFIX}/pretrained/signal_classifier.torch'\n",
        "DEVICE = torch.device('cuda')\n",
        "FRAME_SKIPPING = 2\n",
        "VIDEO_PATH = \"/data/phase_I/video_2.mp4\"\n",
        "VIDEO_WIDTH = 640"
      ],
      "id": "YErqPaC968mO",
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iq2g477V08yw",
        "outputId": "bc5540f5-d656-4f59-a9fa-be29286155f7"
      },
      "source": [
        "def classify(image, bboxes, coord_coef):\n",
        "  def coord_transform(c):\n",
        "    return int(c*coord_coef)\n",
        "  frame_results = {}\n",
        "  for tid, bbox in enumerate(bboxes):\n",
        "      traffic_light = torch.unsqueeze(torch.tensor(image[int(bbox[1]):int(bbox[3]), int(bbox[0]):int(bbox[2]), :], dtype=torch.float32).permute(2,0,1), 0)\n",
        "      tl_classify = torchvision.transforms.functional.resize(traffic_light.to(DEVICE), size=(32,16))\n",
        "      prediction = torch.argmax(tlclassifier(tl_classify)[0]).item()\n",
        "      frame_results[str(tid)] = {\n",
        "          \"coords\": list(map(coord_transform,bbox[:-2])),\n",
        "          \"state\": ['red','yellow','green','unknown'][prediction],\n",
        "          \"affect\": True\n",
        "      }\n",
        "  return frame_results\n",
        "\n",
        "model = torch.hub.load('ultralytics/yolov5', 'custom', path_or_model=OBJECT_DETECTION_MODEL)\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0)\n",
        "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = torch.nn.Linear(630, 64)\n",
        "        self.fc2 = torch.nn.Linear(64, 4)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.log_softmax(x, 1)\n",
        "        return x\n",
        "\n",
        "tlclassifier = torch.load(CLASSIFICATION_MODEL)\n",
        "\n",
        "if torch.cuda.is_available:\n",
        "  model.cuda()\n",
        "  tlclassifier.cuda()\n",
        "\n",
        "video_file = f\"{PREFIX}{VIDEO_PATH}\"\n",
        "probe = ffmpeg.probe(video_file)\n",
        "video_info = next(s for s in probe['streams'] if s['codec_type'] == 'video')\n",
        "old_width = int(video_info['width'])\n",
        "old_height = int(video_info['height'])\n",
        "num_frames = int(eval(video_info['duration'])*eval(video_info['avg_frame_rate']))\n",
        "\n",
        "new_width = 640\n",
        "new_height = int(old_height / (old_width / new_width))\n",
        "\n",
        "coord_coef = old_width / new_width\n",
        "\n",
        "frame_size = new_width*new_height*3\n",
        "process = (\n",
        "    ffmpeg\n",
        "    .input(video_file)\n",
        "    .filter('scale', new_width, new_height)\n",
        "    .output('pipe:', format='rawvideo', pix_fmt='rgb24')\n",
        "    .run_async(pipe_stdout=True)\n",
        ")\n",
        "\n",
        "results = {}\n",
        "\n",
        "for j in range(num_frames):\n",
        "  if j%2==0:\n",
        "    frame = (\n",
        "      np\n",
        "      .frombuffer(process.stdout.read(frame_size), np.uint8)\n",
        "      .reshape((new_height, new_width, 3))\n",
        "    )\n",
        "    prediction = model([frame]).xyxy[0]\n",
        "    predicted_traffic_lights = prediction[prediction[:,5] == 9].cpu().numpy()\n",
        "    if len(predicted_traffic_lights) > 0:\n",
        "      results[str(j)] = classify(frame, predicted_traffic_lights, coord_coef)\n",
        "      results[str(j+1)] = classify(frame, predicted_traffic_lights, coord_coef)\n",
        "\n",
        "json.dump(results, open(\"results.json\",\"w\"))"
      ],
      "id": "Iq2g477V08yw",
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[31m\u001b[1mrequirements:\u001b[0m PyYAML>=5.3.1 not found and is required by YOLOv5, attempting auto-update...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  1    156928  models.common.C3                        [128, 128, 3]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  1    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
            "  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.7/dist-packages (5.4.1)\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per /root/.cache/torch/hub/ultralytics_yolov5_master/requirements.txt\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Summary: 283 layers, 7276605 parameters, 7276605 gradients\n",
            "\n",
            "YOLOv5 🚀 2021-4-1 torch 1.8.1+cu101 CUDA:0 (Tesla T4, 15109.75MB)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Adding autoShape... \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}