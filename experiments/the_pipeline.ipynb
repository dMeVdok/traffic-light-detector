{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "the_pipeline.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZy5GxNSkR_O",
        "outputId": "2f1abfee-a1d8-4f07-f0c4-e30b528098b0"
      },
      "source": [
        "!pip install ffmpeg-python"
      ],
      "id": "GZy5GxNSkR_O",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ffmpeg-python\n",
            "  Downloading https://files.pythonhosted.org/packages/d7/0c/56be52741f75bad4dc6555991fabd2e07b432d333da82c11ad701123888a/ffmpeg_python-0.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from ffmpeg-python) (0.16.0)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lvI9o5T6_dB",
        "outputId": "50bf3350-2bfe-4f92-9009-cf7780d69ecc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "PREFIX = \"/content/drive/MyDrive/traffic-light-detector\""
      ],
      "id": "_lvI9o5T6_dB",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import ffmpeg\n",
        "import PIL\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from tqdm.notebook import tqdm\n",
        "import json\n",
        "import sys\n",
        "\n",
        "# don't forget to download models to pretrained\n",
        "OBJECT_DETECTION_MODEL = f\"{PREFIX}/pretrained/traffic_light_detector.pt\"\n",
        "CLASSIFICATION_MODEL = f\"{PREFIX}/pretrained/signal_classifier.pt\"\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "VIDEO_PATH = sys.argv[-1]\n",
        "VIDEO_WIDTH = 640\n",
        "\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(\n",
        "            in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0\n",
        "        )\n",
        "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = torch.nn.Linear(630, 64)\n",
        "        self.fc2 = torch.nn.Linear(64, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.log_softmax(x, 1)\n",
        "        return x\n",
        "\n",
        "\n",
        "def main():\n",
        "    def classify(image, bboxes, coord_coef):\n",
        "        def coord_transform(c):\n",
        "            return int(c * coord_coef)\n",
        "\n",
        "        frame_results = {}\n",
        "        for tid, bbox in enumerate(bboxes):\n",
        "            traffic_light = torch.unsqueeze(\n",
        "                torch.tensor(\n",
        "                    image[int(bbox[1]) : int(bbox[3]), int(bbox[0]) : int(bbox[2]), :],\n",
        "                    dtype=torch.float32,\n",
        "                ).permute(2, 0, 1),\n",
        "                0,\n",
        "            )\n",
        "            tl_classify = torchvision.transforms.functional.resize(\n",
        "                traffic_light.to(DEVICE), size=(32, 16)\n",
        "            )\n",
        "            prediction = torch.argmax(tlclassifier(tl_classify)[0]).item()\n",
        "            frame_results[str(tid)] = {\n",
        "                \"coords\": list(map(coord_transform, bbox[:-2])),\n",
        "                \"state\": [\"red\", \"yellow\", \"green\", \"unknown\"][prediction],\n",
        "                \"affect\": True\n",
        "                if int((bbox[2] - bbox[0]) / 2) > image.shape[1] / 3\n",
        "                else False,\n",
        "            }\n",
        "        return frame_results\n",
        "\n",
        "    def interpolate(result_before, result_after):\n",
        "        left_frame_results = {}\n",
        "        right_frame_results = {}\n",
        "        lfi, rfi = 0, 0\n",
        "        for tla in result_after.values():\n",
        "            center_after_x, center_after_y = (\n",
        "                tla[\"coords\"][0] + tla[\"coords\"][2]\n",
        "            ) / 2, (tla[\"coords\"][1] + tla[\"coords\"][3]) / 2\n",
        "            for tlb in result_before.values():\n",
        "                center_before_x, center_before_y = (\n",
        "                    tlb[\"coords\"][0] + tlb[\"coords\"][2]\n",
        "                ) / 2, (tlb[\"coords\"][1] + tlb[\"coords\"][3]) / 2\n",
        "                avg_width = (\n",
        "                    (tla[\"coords\"][2] - tla[\"coords\"][0])\n",
        "                    + (tlb[\"coords\"][2] - tlb[\"coords\"][0])\n",
        "                ) / 2\n",
        "                avg_height = (\n",
        "                    (tla[\"coords\"][3] - tla[\"coords\"][1])\n",
        "                    + (tlb[\"coords\"][3] - tlb[\"coords\"][1])\n",
        "                ) / 2\n",
        "                if (\n",
        "                    abs(center_after_x - center_before_x) < avg_width / 2\n",
        "                    and abs(center_after_y - center_before_y) < avg_height / 2\n",
        "                ):\n",
        "                    left_frame_results[str(lfi)] = {\n",
        "                        \"coords\": [\n",
        "                            int(b + (a - b) * 0.333)\n",
        "                            for a, b in zip(tla[\"coords\"], tlb[\"coords\"])\n",
        "                        ],\n",
        "                        \"state\": tla[\"state\"],\n",
        "                        \"affect\": tla[\"affect\"],\n",
        "                    }\n",
        "                    right_frame_results[str(rfi)] = {\n",
        "                        \"coords\": [\n",
        "                            int(b + (a - b) * 0.666)\n",
        "                            for a, b in zip(tla[\"coords\"], tlb[\"coords\"])\n",
        "                        ],\n",
        "                        \"state\": tlb[\"state\"],\n",
        "                        \"affect\": tlb[\"affect\"],\n",
        "                    }\n",
        "                    lfi, rfi = lfi + 1, rfi + 1\n",
        "                    break\n",
        "        return left_frame_results, right_frame_results\n",
        "\n",
        "    model = torch.hub.load(\n",
        "        f\"{PREFIX}/yolov5\",\n",
        "        \"custom\",\n",
        "        source=\"local\",\n",
        "        path_or_model=OBJECT_DETECTION_MODEL,\n",
        "    )\n",
        "\n",
        "    tlclassifier = torch.load(CLASSIFICATION_MODEL)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "        tlclassifier.cuda()\n",
        "    else:\n",
        "        model.cpu()\n",
        "        tlclassifier.cpu()\n",
        "\n",
        "    video_file = f\"{PREFIX}/{VIDEO_PATH}\"\n",
        "    probe = ffmpeg.probe(video_file)\n",
        "    video_info = next(s for s in probe[\"streams\"] if s[\"codec_type\"] == \"video\")\n",
        "    old_width = int(video_info[\"width\"])\n",
        "    old_height = int(video_info[\"height\"])\n",
        "    num_frames = int(eval(video_info[\"duration\"]) * eval(video_info[\"avg_frame_rate\"]))\n",
        "\n",
        "    new_width = VIDEO_WIDTH\n",
        "    new_height = int(old_height / (old_width / new_width))\n",
        "\n",
        "    coord_coef = old_width / new_width\n",
        "\n",
        "    frame_size = new_width * new_height * 3\n",
        "    process = (\n",
        "        ffmpeg.input(video_file)\n",
        "        .filter(\"scale\", new_width, new_height)\n",
        "        .output(\"pipe:\", format=\"rawvideo\", pix_fmt=\"rgb24\")\n",
        "        .run_async(pipe_stdout=True)\n",
        "    )\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for j in range(num_frames):\n",
        "        if j % 3 == 0:\n",
        "            frame = np.frombuffer(process.stdout.read(frame_size), np.uint8).reshape(\n",
        "                (new_height, new_width, 3)\n",
        "            )\n",
        "            prediction = model([frame]).xyxy[0]\n",
        "            predicted_traffic_lights = prediction[prediction[:, 5] == 9].cpu().numpy()\n",
        "            if len(predicted_traffic_lights) > 0:\n",
        "                results[str(j)] = classify(frame, predicted_traffic_lights, coord_coef)\n",
        "                if (j > 2) and (str(j - 3) in results):\n",
        "                    results[str(j - 2)], results[str(j - 1)] = interpolate(\n",
        "                        results[str(j - 3)], results[str(j)]\n",
        "                    )\n",
        "        else:\n",
        "            process.stdout.read(frame_size)\n",
        "\n",
        "    json.dump(results, open(f\"{VIDEO_PATH}.json\", \"w\"))\n",
        "\n",
        "\n",
        "main()"
      ]
    }
  ]
}