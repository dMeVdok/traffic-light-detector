{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "asvL1QtI6N48",
    "outputId": "42f00ca2-28c8-4dde-c1e4-1efa4b499b96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_lightning\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/01/5df6324efdc3f79025ea7eaf19478936c401a16dae4fd3fbd29f7d426974/pytorch_lightning-1.2.6-py3-none-any.whl (829kB)\n",
      "\u001b[K     |████████████████████████████████| 839kB 8.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.4.1)\n",
      "Collecting PyYAML!=5.4.*,>=5.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
      "\u001b[K     |████████████████████████████████| 276kB 17.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.19.5)\n",
      "Collecting torchmetrics>=0.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/42/d984612cabf005a265aa99c8d4ab2958e37b753aafb12f31c81df38751c8/torchmetrics-0.2.0-py3-none-any.whl (176kB)\n",
      "\u001b[K     |████████████████████████████████| 184kB 30.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.41.1)\n",
      "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.8.1+cu101)\n",
      "Collecting fsspec[http]>=0.8.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/0d/a6bfee0ddf47b254286b9bd574e6f50978c69897647ae15b14230711806e/fsspec-0.8.7-py3-none-any.whl (103kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 30.1MB/s \n",
      "\u001b[?25hCollecting future>=0.17.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "\u001b[K     |████████████████████████████████| 829kB 28.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.32.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.12.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.28.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (54.2.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.12.4)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.36.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.23.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch_lightning) (3.7.4.3)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning) (3.8.1)\n",
      "Collecting aiohttp; extra == \"http\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 53.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (2.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->fsspec[http]>=0.8.1->pytorch_lightning) (3.4.1)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 51.0MB/s \n",
      "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (20.3.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 41.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.0)\n",
      "Building wheels for collected packages: PyYAML, future\n",
      "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44620 sha256=19862e7a58c9de16d706a0c0aeac52a7019bd0e275aca8a791649044e87bbfed\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
      "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=338ef6f3525426089949638ced55707419cdd6fd4ad508c5caa876bee744c48f\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
      "Successfully built PyYAML future\n",
      "Installing collected packages: PyYAML, torchmetrics, multidict, async-timeout, yarl, aiohttp, fsspec, future, pytorch-lightning\n",
      "  Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "  Found existing installation: future 0.16.0\n",
      "    Uninstalling future-0.16.0:\n",
      "      Successfully uninstalled future-0.16.0\n",
      "Successfully installed PyYAML-5.3.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 fsspec-0.8.7 future-0.18.2 multidict-5.1.0 pytorch-lightning-1.2.6 torchmetrics-0.2.0 yarl-1.6.3\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
      "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.8.1+cu101)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n",
      "Collecting ffmpeg-python\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/0c/56be52741f75bad4dc6555991fabd2e07b432d333da82c11ad701123888a/ffmpeg_python-0.2.0-py3-none-any.whl\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from ffmpeg-python) (0.18.2)\n",
      "Installing collected packages: ffmpeg-python\n",
      "Successfully installed ffmpeg-python-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_lightning\n",
    "!pip install torchvision\n",
    "!pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Gndo_l_x6TXX"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'InterpolationMode' from 'torchvision.transforms.functional' (/Users/dmevdok/anaconda3/envs/tldetector/lib/python3.8/site-packages/torchvision/transforms/functional.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-637f3bef4579>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterpolationMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'InterpolationMode' from 'torchvision.transforms.functional' (/Users/dmevdok/anaconda3/envs/tldetector/lib/python3.8/site-packages/torchvision/transforms/functional.py)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch, time, torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import time, ffmpeg\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.transforms.functional import InterpolationMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iDGUYjBHYK1b",
    "outputId": "0b4f0c30-9927-4005-c8e8-7a54fc0bbe7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fexperimentsandconfigs%20https%3a%2f%2fwww.googleapis.com%2fauth%2fphotos.native&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "4/1AY0e-g7xs_V87UoT3fDWNgSqBD0vSwQBaPg8H7K7OIBpuylPfA6_2-CJP6o\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive \n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-bAM57bYesQ"
   },
   "outputs": [],
   "source": [
    "video_path = '/content/drive/MyDrive/Lara-video.mpg'\n",
    "labels_path = '/content/drive/MyDrive/Lara-labels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSoRcdfSScv6"
   },
   "outputs": [],
   "source": [
    "bbox = torch.Tensor([\n",
    "    [0,0,0,0,-1],\n",
    "    [0,0,0,0,-1],\n",
    "    [0,0,0,0,-1],\n",
    "    [0,0,0,0,-1],\n",
    "    [0,2,0,2,1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-TpiCbqTR-pi",
    "outputId": "6a69e28d-b1f0-4185-d22a-43de7172222d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "[[0, 0, 1, 1, -1], [0, 0, 1, 1, -1], [0, 0, 1, 1, -1], [0, 0, 1, 1, -1], [0, 2, 0, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "for i in range(5):\n",
    "    bbox_ = bbox[i]\n",
    "\n",
    "    if bbox_[4]==-1:\n",
    "        x1,y1,x2,y2,subtype = 0,0,1,1,-1\n",
    "        print(1)\n",
    "    else:\n",
    "        x1,y1,x2,y2,subtype = bbox_\n",
    "        x1,y1,x2,y2,subtype = int(x1),int(y1),int(x2),int(y2),int(subtype)\n",
    "        print(2)\n",
    "\n",
    "    samples.append([x1,y1,x2,y2,subtype])\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Ot4DHXm-Vk0"
   },
   "outputs": [],
   "source": [
    "class CropImage(object):\n",
    "    def __init__(self):\n",
    "      pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image=sample['image']\n",
    "        bbox=sample['bbox']\n",
    "        #print(bbox.shape)\n",
    "        samples = []\n",
    "        for i in range(5):\n",
    "            bbox_ = bbox[i]\n",
    "\n",
    "            if bbox_[4]==-1:\n",
    "                x1,y1,x2,y2,subtype = 0,0,1,1,-1\n",
    "            else:\n",
    "                x1,y1,x2,y2,subtype = bbox_\n",
    "                x1,y1,x2,y2,subtype = int(x1),int(y1),int(x2),int(y2),int(subtype)\n",
    "\n",
    "            top, left, height, width = y1,x1,y2-y1,x2-x1\n",
    "            if top < 0:\n",
    "                top = 0\n",
    "            if left < 0:\n",
    "                left = 0\n",
    "            image_ = transforms.functional.resized_crop(image,top=top, left=left, height=height,\\\n",
    "                                                        width=width, size=(32,32))\n",
    "            sample['image'] = image_\n",
    "            sample['bbox'] = subtype\n",
    "            if subtype != -1:\n",
    "                print('not -1')\n",
    "            samples.append(sample)\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dg31vZLAVAvn"
   },
   "outputs": [],
   "source": [
    "class TrafficLightsDataset(torch.utils.data.IterableDataset):\n",
    "    \"\"\"Traffic lights dataset.\"\"\"\n",
    "      \n",
    "    def ffmpeg_process(self, video_file):\n",
    "        probe = ffmpeg.probe(video_file)\n",
    "        video_info = next(s for s in probe['streams'] if s['codec_type'] == 'video')\n",
    "        old_width = int(video_info['width'])\n",
    "        old_height = int(video_info['height'])\n",
    "        num_frames = int(eval(video_info['duration'])*eval(video_info['avg_frame_rate']))\n",
    "        \n",
    "        frame_size = old_width*old_height*3\n",
    "        #print(frame_idx)\n",
    "        process = (\n",
    "            ffmpeg\n",
    "            .input(video_file)\n",
    "            .filter('scale', old_width, old_height)\n",
    "            .output('pipe:', format='rawvideo', pix_fmt='rgb24')\n",
    "            .run_async(pipe_stdout=True)\n",
    "        )\n",
    "\n",
    "        return process, num_frames, {'frame_size': frame_size, \n",
    "                                     'height': old_height, \n",
    "                                     'width': old_width}\n",
    "\n",
    "    def labels_loader(self, frame_index, max_traffic_lights=10):\n",
    "        df = self.labels_df\n",
    "        frame_labels = df[(df.frameindex==frame_index) & (df.type=='traffic-light')]\n",
    "        signal_mapping = {\"stop\": 0, \"warning\": 1, \"go\": 2, \"ambiguous\": 3}\n",
    "        tl_signals = frame_labels.subtype.replace(signal_mapping).values\n",
    "        frame_labels = np.hstack([frame_labels.values[:, 1:5], tl_signals.reshape(-1,1)]).astype(dtype=np.int32)\n",
    "        frame_labels_padded = np.zeros((5,5))\n",
    "        frame_labels_padded[:, 4] = -1\n",
    "        frame_labels_padded[:frame_labels.shape[0], :frame_labels.shape[1]] = frame_labels\n",
    "        return frame_labels_padded\n",
    "\n",
    "    def __init__(self, video_file, labels_file):\n",
    "        self.process, self.num_frames, self.video_meta = self.ffmpeg_process(video_file)\n",
    "        self.cnt = 0\n",
    "        self.labels_df = pd.read_csv(labels_file, sep=' ')\n",
    "        self.labels_df.frameindex = self.labels_df.frameindex.astype(int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_frames\n",
    "\n",
    "    def __iter__(self):\n",
    "                                     \n",
    "        frame_size = self.video_meta['frame_size']\n",
    "        old_width = self.video_meta['width']\n",
    "        old_height = self.video_meta['height']\n",
    "        for i in range(self.num_frames):\n",
    "            bbox = self.labels_loader(i)\n",
    "            frame = (\n",
    "                np\n",
    "                .frombuffer(self.process.stdout.read(frame_size), np.uint8)\n",
    "                .reshape(old_height, old_width, 3)\n",
    "            )\n",
    "            frame = np.transpose(frame, (2,0,1))\n",
    "            image = torch.tensor(frame, dtype=torch.int32)\n",
    "            sample = {'image': image, 'bbox': bbox}\n",
    "\n",
    "            image=sample['image']\n",
    "            bbox=sample['bbox']\n",
    "\n",
    "            for j in range(5):\n",
    "                bbox_ = bbox[j]\n",
    "\n",
    "                if bbox_[4]==-1:\n",
    "                    x1,y1,x2,y2,subtype = 0,0,1,1,-1\n",
    "                else:\n",
    "                    x1,y1,x2,y2,subtype = bbox_\n",
    "                    x1,y1,x2,y2,subtype = int(x1),int(y1),int(x2),int(y2),int(subtype)\n",
    "\n",
    "                top, left, height, width = y1-5,x1-5,y2-y1+10,x2-x1+10\n",
    "                if top < 0:\n",
    "                    top = 0\n",
    "                if left < 0:\n",
    "                    left = 0\n",
    "                image_ = transforms.functional.resized_crop(image,top=top, left=left, height=height,\\\n",
    "                                                            width=width, size=(32,16))\n",
    "                sample['image'] = image_\n",
    "                sample['bbox'] = subtype\n",
    "                if subtype != -1:\n",
    "                    yield (image_, subtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlB5l8_TJB23"
   },
   "outputs": [],
   "source": [
    "def img_show(img, opencv=False):\n",
    "    if opencv:\n",
    "        img_to_show = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    else:\n",
    "        img_to_show = img\n",
    "    plt.figure(dpi=50)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img_to_show)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2HBdUPsU2Ero",
    "outputId": "2da3c639-1c2a-4f2c-eb6b-fc6e09eecd8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=630, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(630, 64)\n",
    "        self.fc2 = nn.Linear(64, 4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, 1)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULiGEScBXRMm"
   },
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, optimizer):\n",
    "    X_train = X_train.float()\n",
    "    y_train = y_train\n",
    "    #y_train[y_train==-1] = 4\n",
    "    optimizer.zero_grad()\n",
    "    out = model(X_train)\n",
    "    loss = F.nll_loss(out,y_train )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def test(model, X_test, y_test,):\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from pytorch_lightning.metrics.classification import Accuracy\n",
    "    confmat = Accuracy()\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    with torch.no_grad():\n",
    "        X_test = X_test.float()\n",
    "        y_test = y_test\n",
    "        out = model(X_test)\n",
    "        loss = F.nll_loss(out, y_test, reduction='sum')\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        c = (predicted == y_test).squeeze()\n",
    "\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    print(np.unique(predicted))\n",
    "    acc = accuracy_score(y_test, predicted)\n",
    "    cr = classification_report(y_test, predicted)\n",
    "    return test_loss / y_test.shape[0], cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "DW7QJLXRISHo"
   },
   "outputs": [],
   "source": [
    "def get_loaders(video_path, labels_path, batch_size=128, num_workers=1):\n",
    "    dataset = TrafficLightsDataset(video_path, labels_path)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "    return train_loader, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "F3aDjLtHe67M"
   },
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    CropImage(),\n",
    "])\n",
    "\n",
    "train_loader,_ = get_loaders(video_path, labels_path, batch_size=10000, num_workers=1)\n",
    "sample = iter(train_loader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SyfL-fOpBjpD",
    "outputId": "df3a87ac-cc2e-4e69-e2b8-bd67b378522a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([5267])\n",
      "1 torch.Size([58])\n",
      "2 torch.Size([3381])\n",
      "3 torch.Size([449])\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,4):\n",
    "    print(i, sample[1][sample[1]==i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "otn9Rc4Jh2st"
   },
   "outputs": [],
   "source": [
    "image = sample[0]\n",
    "bbox = sample[1]\n",
    "sample = {'image':image, 'bbox': bbox}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "mx6_n9TdRU9M"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "p = np.random.permutation(len(sample['image']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "mHfmf--tRllI"
   },
   "outputs": [],
   "source": [
    "sample['image'] = sample['image'][p,...]\n",
    "sample['bbox'] = sample['bbox'][p,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8XLr14yWB5f-",
    "outputId": "d44033d2-713b-4b0e-b3e8-89293b892f7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9155, 3, 32, 16])"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "durzVKPuCiD3",
    "outputId": "3c377e1f-8100-4da4-d7cf-f59dddd5a900"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(sample['bbox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "okXgGr_gCP_l",
    "outputId": "6256f52b-1cd7-4d77-eadb-cb0c0f7236a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3]), array([0, 1, 2, 3]))"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train), np.unique(sample['bbox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "R9ojfsQ1J5sN"
   },
   "outputs": [],
   "source": [
    "X, y = sample['image'], sample['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "NS5__J3SJsRw"
   },
   "outputs": [],
   "source": [
    "y_0 = y[y==0]\n",
    "y_2 = y[y==2]\n",
    "\n",
    "X_0 = X[y==0]\n",
    "X_2 = X[y==2]\n",
    "\n",
    "indicies_0 = torch.randperm(len(y_0))\n",
    "indicies_2 = torch.randperm(len(y_2))\n",
    "\n",
    "y_0 = y_0[indicies_0][0:500]\n",
    "y_2 = y_2[indicies_2][0:700]\n",
    "\n",
    "X_0 = X_0[indicies_0][0:500]\n",
    "X_2 = X_2[indicies_2][0:700]\n",
    "\n",
    "X = X[(y!=0) & (y!=2)]\n",
    "y = y[(y!=0) & (y!=2)]\n",
    "\n",
    "X = torch.cat([X,X_0,X_2])\n",
    "y = torch.cat([y,y_0,y_2])\n",
    "p = np.random.permutation(y.shape[0])\n",
    "\n",
    "X = X[p,...]\n",
    "y = y[p,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ag16jIi5KSyW",
    "outputId": "907b7b2c-1e9e-430a-af5e-ceb758c2ef4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([500])\n",
      "1 torch.Size([58])\n",
      "2 torch.Size([700])\n",
      "3 torch.Size([449])\n",
      "4 torch.Size([0])\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(i, y[y==i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQkhIlOlK5Rp",
    "outputId": "c9f3ecbb-e042-4a99-c2f5-dd9209ffd659"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1707"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "hFdzEVn_KKtO"
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = X[0:1200,...],y[0:1200,...],X[1200:-1,...],y[1200:-1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Gl9FmBWU2iq",
    "outputId": "46370188-eef4-4000-c949-d04eb1e889dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([351])\n",
      "1 torch.Size([35])\n",
      "2 torch.Size([490])\n",
      "3 torch.Size([324])\n",
      "4 torch.Size([0])\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(i, y_train[y_train==i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ma6AHDaCl7JD",
    "outputId": "f221440b-5bb6-479b-d800-583cef17212f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([149])\n",
      "1 torch.Size([23])\n",
      "2 torch.Size([210])\n",
      "3 torch.Size([124])\n",
      "4 torch.Size([0])\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(i, y_test[y_test==i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "hAqKB3JEmGmw"
   },
   "outputs": [],
   "source": [
    "yellow = X_test[y_test==3]\n",
    "yellow_0 = np.transpose(yellow[10], (1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "fZ2zPvAOmMGf",
    "outputId": "acbabb00-0019-4480-d492-20734e889019"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_show(yellow_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QTdQZL9sbHZb",
    "outputId": "05e7f30f-e995-4cd7-ad6a-2998e9de0a0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3]), array([0, 1, 2, 3]))"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train), np.unique(sample['bbox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v87DdQNKYFWW",
    "outputId": "dd25d38e-3c2b-4b8c-f6ce-d380876ca70e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([351])\n",
      "1 torch.Size([35])\n",
      "2 torch.Size([490])\n",
      "3 torch.Size([324])\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(i, y_train[y_train==i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3EX7U-xRGRw7",
    "outputId": "9a6ad52c-32a8-4fa9-ac69-4ded99a7798e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([149])\n",
      "1 torch.Size([23])\n",
      "2 torch.Size([210])\n",
      "3 torch.Size([124])\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(i, y_test[y_test==i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LoTNIQBzhLBq",
    "outputId": "4d8598bf-6d07-4371-f711-989a7ea85233"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 3]\n",
      "Epoch: 0, test loss: 6.65277557976161\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.88      0.55       149\n",
      "           1       0.29      0.43      0.35        23\n",
      "           2       0.00      0.00      0.00       210\n",
      "           3       0.26      0.29      0.27       124\n",
      "\n",
      "    accuracy                           0.35       506\n",
      "   macro avg       0.24      0.40      0.29       506\n",
      "weighted avg       0.19      0.35      0.24       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 1, test loss: 3.711013733633893\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.07      0.12       149\n",
      "           1       0.25      0.91      0.39        23\n",
      "           2       0.52      0.12      0.19       210\n",
      "           3       0.28      0.76      0.40       124\n",
      "\n",
      "    accuracy                           0.30       506\n",
      "   macro avg       0.35      0.47      0.28       506\n",
      "weighted avg       0.40      0.30      0.23       506\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n",
      "Epoch: 2, test loss: 3.490768492928607\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.01      0.03       149\n",
      "           1       0.10      0.13      0.12        23\n",
      "           2       0.36      0.71      0.48       210\n",
      "           3       0.15      0.07      0.10       124\n",
      "\n",
      "    accuracy                           0.32       506\n",
      "   macro avg       0.32      0.23      0.18       506\n",
      "weighted avg       0.39      0.32      0.24       506\n",
      "\n",
      "[1 2 3]\n",
      "Epoch: 3, test loss: 4.276405304316946\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       149\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.41      0.98      0.58       210\n",
      "           3       0.33      0.02      0.03       124\n",
      "\n",
      "    accuracy                           0.41       506\n",
      "   macro avg       0.19      0.25      0.15       506\n",
      "weighted avg       0.25      0.41      0.25       506\n",
      "\n",
      "[1 2 3]\n",
      "Epoch: 4, test loss: 3.526853538784585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       149\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.41      0.98      0.58       210\n",
      "           3       0.40      0.02      0.03       124\n",
      "\n",
      "    accuracy                           0.41       506\n",
      "   macro avg       0.20      0.25      0.15       506\n",
      "weighted avg       0.27      0.41      0.25       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 5, test loss: 2.1389664359714673\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.35      0.52       149\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.45      0.94      0.61       210\n",
      "           3       0.21      0.02      0.04       124\n",
      "\n",
      "    accuracy                           0.50       506\n",
      "   macro avg       0.42      0.33      0.29       506\n",
      "weighted avg       0.54      0.50      0.42       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 6, test loss: 1.254379453395195\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.73      0.84       149\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.48      0.66      0.56       210\n",
      "           3       0.25      0.17      0.20       124\n",
      "\n",
      "    accuracy                           0.53       506\n",
      "   macro avg       0.42      0.39      0.40       506\n",
      "weighted avg       0.55      0.53      0.53       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 7, test loss: 1.2779528953341157\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.83      0.77       149\n",
      "           1       0.11      0.17      0.13        23\n",
      "           2       0.60      0.34      0.43       210\n",
      "           3       0.34      0.49      0.40       124\n",
      "\n",
      "    accuracy                           0.51       506\n",
      "   macro avg       0.44      0.46      0.43       506\n",
      "weighted avg       0.55      0.51      0.51       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 8, test loss: 1.8149546747622283\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.93      0.65       149\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.61      0.05      0.10       210\n",
      "           3       0.33      0.46      0.39       124\n",
      "\n",
      "    accuracy                           0.41       506\n",
      "   macro avg       0.36      0.36      0.28       506\n",
      "weighted avg       0.48      0.41      0.33       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 9, test loss: 2.1098490477550644\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.95      0.60       149\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.67      0.03      0.05       210\n",
      "           3       0.34      0.39      0.36       124\n",
      "\n",
      "    accuracy                           0.39       506\n",
      "   macro avg       0.36      0.34      0.26       506\n",
      "weighted avg       0.49      0.39      0.29       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 10, test loss: 2.084614267462327\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.95      0.60       149\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.78      0.03      0.06       210\n",
      "           3       0.35      0.38      0.36       124\n",
      "\n",
      "    accuracy                           0.39       506\n",
      "   macro avg       0.39      0.34      0.26       506\n",
      "weighted avg       0.54      0.39      0.29       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 11, test loss: 1.8839065491446394\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.95      0.61       149\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.60      0.04      0.08       210\n",
      "           3       0.36      0.38      0.37       124\n",
      "\n",
      "    accuracy                           0.39       506\n",
      "   macro avg       0.35      0.34      0.26       506\n",
      "weighted avg       0.47      0.39      0.30       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 12, test loss: 1.6501306827831645\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.93      0.62       149\n",
      "           1       0.04      0.09      0.05        23\n",
      "           2       0.58      0.07      0.12       210\n",
      "           3       0.34      0.35      0.35       124\n",
      "\n",
      "    accuracy                           0.39       506\n",
      "   macro avg       0.36      0.36      0.28       506\n",
      "weighted avg       0.46      0.39      0.32       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 13, test loss: 1.4867665890177248\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.89      0.65       149\n",
      "           1       0.01      0.04      0.02        23\n",
      "           2       0.63      0.14      0.23       210\n",
      "           3       0.40      0.40      0.40       124\n",
      "\n",
      "    accuracy                           0.42       506\n",
      "   macro avg       0.39      0.37      0.32       506\n",
      "weighted avg       0.51      0.42      0.38       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 14, test loss: 1.386855656921628\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.87      0.69       149\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.56      0.19      0.28       210\n",
      "           3       0.38      0.39      0.39       124\n",
      "\n",
      "    accuracy                           0.43       506\n",
      "   macro avg       0.38      0.36      0.34       506\n",
      "weighted avg       0.49      0.43      0.41       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 15, test loss: 1.3113451569447876\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.87      0.72       149\n",
      "           1       0.03      0.13      0.05        23\n",
      "           2       0.57      0.20      0.30       210\n",
      "           3       0.42      0.44      0.43       124\n",
      "\n",
      "    accuracy                           0.45       506\n",
      "   macro avg       0.41      0.41      0.37       506\n",
      "weighted avg       0.52      0.45      0.44       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 16, test loss: 1.2516756924715908\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.87      0.74       149\n",
      "           1       0.05      0.17      0.07        23\n",
      "           2       0.56      0.23      0.32       210\n",
      "           3       0.44      0.47      0.45       124\n",
      "\n",
      "    accuracy                           0.47       506\n",
      "   macro avg       0.42      0.44      0.40       506\n",
      "weighted avg       0.53      0.47      0.47       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 17, test loss: 1.2008025278687007\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.86      0.74       149\n",
      "           1       0.05      0.17      0.08        23\n",
      "           2       0.56      0.25      0.34       210\n",
      "           3       0.46      0.50      0.48       124\n",
      "\n",
      "    accuracy                           0.49       506\n",
      "   macro avg       0.43      0.45      0.41       506\n",
      "weighted avg       0.54      0.49      0.48       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 18, test loss: 1.1613271358927248\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.87      0.75       149\n",
      "           1       0.05      0.17      0.08        23\n",
      "           2       0.57      0.26      0.35       210\n",
      "           3       0.45      0.49      0.47       124\n",
      "\n",
      "    accuracy                           0.49       506\n",
      "   macro avg       0.43      0.45      0.41       506\n",
      "weighted avg       0.54      0.49      0.48       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 19, test loss: 1.1306691527837822\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.87      0.75       149\n",
      "           1       0.06      0.17      0.09        23\n",
      "           2       0.58      0.29      0.38       210\n",
      "           3       0.45      0.52      0.48       124\n",
      "\n",
      "    accuracy                           0.51       506\n",
      "   macro avg       0.44      0.46      0.43       506\n",
      "weighted avg       0.55      0.51      0.50       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 20, test loss: 1.1055833416965168\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.87      0.77       149\n",
      "           1       0.07      0.17      0.10        23\n",
      "           2       0.58      0.31      0.40       210\n",
      "           3       0.46      0.54      0.50       124\n",
      "\n",
      "    accuracy                           0.52       506\n",
      "   macro avg       0.45      0.47      0.44       506\n",
      "weighted avg       0.56      0.52      0.52       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 21, test loss: 1.0840227330626235\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.86      0.76       149\n",
      "           1       0.08      0.17      0.11        23\n",
      "           2       0.58      0.33      0.42       210\n",
      "           3       0.45      0.54      0.49       124\n",
      "\n",
      "    accuracy                           0.53       506\n",
      "   macro avg       0.45      0.48      0.45       506\n",
      "weighted avg       0.56      0.53      0.52       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 22, test loss: 1.0650042507488267\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.85      0.77       149\n",
      "           1       0.08      0.17      0.11        23\n",
      "           2       0.57      0.35      0.44       210\n",
      "           3       0.46      0.54      0.49       124\n",
      "\n",
      "    accuracy                           0.54       506\n",
      "   macro avg       0.45      0.48      0.45       506\n",
      "weighted avg       0.56      0.54      0.53       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 23, test loss: 1.0477969203541873\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.85      0.77       149\n",
      "           1       0.10      0.17      0.13        23\n",
      "           2       0.57      0.38      0.45       210\n",
      "           3       0.46      0.54      0.50       124\n",
      "\n",
      "    accuracy                           0.55       506\n",
      "   macro avg       0.46      0.49      0.46       506\n",
      "weighted avg       0.56      0.55      0.54       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 24, test loss: 1.0322296986937993\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.85      0.77       149\n",
      "           1       0.10      0.17      0.13        23\n",
      "           2       0.57      0.39      0.46       210\n",
      "           3       0.47      0.54      0.50       124\n",
      "\n",
      "    accuracy                           0.55       506\n",
      "   macro avg       0.46      0.49      0.47       506\n",
      "weighted avg       0.56      0.55      0.55       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 25, test loss: 1.0179433709547925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.85      0.77       149\n",
      "           1       0.10      0.17      0.13        23\n",
      "           2       0.57      0.40      0.47       210\n",
      "           3       0.48      0.54      0.51       124\n",
      "\n",
      "    accuracy                           0.56       506\n",
      "   macro avg       0.47      0.49      0.47       506\n",
      "weighted avg       0.57      0.56      0.55       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 26, test loss: 1.0045769732931387\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.84      0.77       149\n",
      "           1       0.10      0.17      0.13        23\n",
      "           2       0.58      0.42      0.48       210\n",
      "           3       0.48      0.54      0.51       124\n",
      "\n",
      "    accuracy                           0.56       506\n",
      "   macro avg       0.47      0.49      0.47       506\n",
      "weighted avg       0.57      0.56      0.56       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 27, test loss: 0.9917617013803113\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.84      0.78       149\n",
      "           1       0.10      0.17      0.13        23\n",
      "           2       0.57      0.42      0.48       210\n",
      "           3       0.48      0.54      0.51       124\n",
      "\n",
      "    accuracy                           0.56       506\n",
      "   macro avg       0.47      0.49      0.47       506\n",
      "weighted avg       0.57      0.56      0.56       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 28, test loss: 0.9794461698871356\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.84      0.78       149\n",
      "           1       0.11      0.17      0.13        23\n",
      "           2       0.59      0.46      0.51       210\n",
      "           3       0.49      0.54      0.52       124\n",
      "\n",
      "    accuracy                           0.58       506\n",
      "   macro avg       0.48      0.50      0.49       506\n",
      "weighted avg       0.59      0.58      0.58       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 29, test loss: 0.9675711529999382\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.84      0.79       149\n",
      "           1       0.11      0.17      0.14        23\n",
      "           2       0.60      0.49      0.54       210\n",
      "           3       0.50      0.53      0.52       124\n",
      "\n",
      "    accuracy                           0.59       506\n",
      "   macro avg       0.49      0.51      0.50       506\n",
      "weighted avg       0.60      0.59      0.59       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 30, test loss: 0.9561370728986537\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.83      0.79       149\n",
      "           1       0.12      0.17      0.14        23\n",
      "           2       0.60      0.51      0.55       210\n",
      "           3       0.52      0.52      0.52       124\n",
      "\n",
      "    accuracy                           0.59       506\n",
      "   macro avg       0.50      0.51      0.50       506\n",
      "weighted avg       0.60      0.59      0.60       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 31, test loss: 0.9450545480599988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.83      0.79       149\n",
      "           1       0.12      0.17      0.14        23\n",
      "           2       0.61      0.53      0.57       210\n",
      "           3       0.52      0.52      0.52       124\n",
      "\n",
      "    accuracy                           0.60       506\n",
      "   macro avg       0.50      0.51      0.50       506\n",
      "weighted avg       0.61      0.60      0.60       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 32, test loss: 0.9341793135692008\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.83      0.79       149\n",
      "           1       0.12      0.17      0.15        23\n",
      "           2       0.60      0.54      0.57       210\n",
      "           3       0.52      0.51      0.51       124\n",
      "\n",
      "    accuracy                           0.60       506\n",
      "   macro avg       0.50      0.51      0.50       506\n",
      "weighted avg       0.60      0.60      0.60       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 33, test loss: 0.9234975581112587\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.83      0.79       149\n",
      "           1       0.14      0.17      0.15        23\n",
      "           2       0.59      0.56      0.58       210\n",
      "           3       0.51      0.48      0.50       124\n",
      "\n",
      "    accuracy                           0.60       506\n",
      "   macro avg       0.50      0.51      0.51       506\n",
      "weighted avg       0.60      0.60      0.60       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 34, test loss: 0.9132327354943799\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.83      0.80       149\n",
      "           1       0.15      0.17      0.16        23\n",
      "           2       0.61      0.60      0.60       210\n",
      "           3       0.54      0.48      0.51       124\n",
      "\n",
      "    accuracy                           0.62       506\n",
      "   macro avg       0.52      0.52      0.52       506\n",
      "weighted avg       0.62      0.62      0.62       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 35, test loss: 0.9035134899757596\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.80       149\n",
      "           1       0.12      0.13      0.12        23\n",
      "           2       0.61      0.62      0.61       210\n",
      "           3       0.54      0.49      0.52       124\n",
      "\n",
      "    accuracy                           0.62       506\n",
      "   macro avg       0.51      0.52      0.51       506\n",
      "weighted avg       0.62      0.62      0.62       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 36, test loss: 0.8943329460536067\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.82      0.81       149\n",
      "           1       0.12      0.13      0.12        23\n",
      "           2       0.61      0.64      0.62       210\n",
      "           3       0.54      0.47      0.50       124\n",
      "\n",
      "    accuracy                           0.63       506\n",
      "   macro avg       0.52      0.51      0.51       506\n",
      "weighted avg       0.63      0.63      0.63       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 37, test loss: 0.8855857396785449\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.82       149\n",
      "           1       0.09      0.09      0.09        23\n",
      "           2       0.62      0.68      0.65       210\n",
      "           3       0.55      0.45      0.50       124\n",
      "\n",
      "    accuracy                           0.64       506\n",
      "   macro avg       0.52      0.51      0.51       506\n",
      "weighted avg       0.63      0.64      0.63       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 38, test loss: 0.8774183672878582\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       149\n",
      "           1       0.10      0.09      0.09        23\n",
      "           2       0.61      0.70      0.65       210\n",
      "           3       0.57      0.44      0.50       124\n",
      "\n",
      "    accuracy                           0.64       506\n",
      "   macro avg       0.52      0.51      0.52       506\n",
      "weighted avg       0.64      0.64      0.64       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 39, test loss: 0.8697236554895936\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.83      0.82       149\n",
      "           1       0.06      0.04      0.05        23\n",
      "           2       0.61      0.71      0.66       210\n",
      "           3       0.58      0.43      0.49       124\n",
      "\n",
      "    accuracy                           0.65       506\n",
      "   macro avg       0.52      0.50      0.51       506\n",
      "weighted avg       0.64      0.65      0.64       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 40, test loss: 0.8623315863929718\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.83      0.83       149\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.60      0.74      0.66       210\n",
      "           3       0.59      0.40      0.47       124\n",
      "\n",
      "    accuracy                           0.65       506\n",
      "   macro avg       0.51      0.49      0.49       506\n",
      "weighted avg       0.64      0.65      0.64       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 41, test loss: 0.855213331139606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.83      0.83       149\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.60      0.75      0.67       210\n",
      "           3       0.59      0.39      0.47       124\n",
      "\n",
      "    accuracy                           0.65       506\n",
      "   macro avg       0.51      0.49      0.49       506\n",
      "weighted avg       0.64      0.65      0.64       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 42, test loss: 0.8485393901116292\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84       149\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.60      0.75      0.67       210\n",
      "           3       0.58      0.38      0.46       124\n",
      "\n",
      "    accuracy                           0.65       506\n",
      "   macro avg       0.51      0.49      0.49       506\n",
      "weighted avg       0.64      0.65      0.64       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 43, test loss: 0.8421878739307992\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.81      0.83       149\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.59      0.75      0.66       210\n",
      "           3       0.56      0.36      0.44       124\n",
      "\n",
      "    accuracy                           0.64       506\n",
      "   macro avg       0.50      0.48      0.48       506\n",
      "weighted avg       0.63      0.64      0.63       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 44, test loss: 0.836008426229002\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.81      0.83       149\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.60      0.78      0.67       210\n",
      "           3       0.57      0.36      0.44       124\n",
      "\n",
      "    accuracy                           0.65       506\n",
      "   macro avg       0.51      0.49      0.49       506\n",
      "weighted avg       0.64      0.65      0.63       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 45, test loss: 0.8298553346174037\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.81      0.84       149\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.60      0.79      0.68       210\n",
      "           3       0.58      0.36      0.45       124\n",
      "\n",
      "    accuracy                           0.65       506\n",
      "   macro avg       0.51      0.49      0.49       506\n",
      "weighted avg       0.64      0.65      0.64       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 46, test loss: 0.8237330018296072\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.82      0.85       149\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.60      0.80      0.68       210\n",
      "           3       0.59      0.37      0.46       124\n",
      "\n",
      "    accuracy                           0.66       506\n",
      "   macro avg       0.52      0.50      0.50       506\n",
      "weighted avg       0.65      0.66      0.65       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 47, test loss: 0.8175162213593132\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.82      0.85       149\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.60      0.80      0.68       210\n",
      "           3       0.59      0.37      0.46       124\n",
      "\n",
      "    accuracy                           0.66       506\n",
      "   macro avg       0.52      0.50      0.50       506\n",
      "weighted avg       0.65      0.66      0.65       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 48, test loss: 0.8112005301614995\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.82      0.85       149\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.60      0.80      0.68       210\n",
      "           3       0.60      0.37      0.46       124\n",
      "\n",
      "    accuracy                           0.66       506\n",
      "   macro avg       0.52      0.50      0.50       506\n",
      "weighted avg       0.65      0.66      0.65       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 49, test loss: 0.8048082434612772\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86       149\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.60      0.81      0.69       210\n",
      "           3       0.61      0.37      0.46       124\n",
      "\n",
      "    accuracy                           0.67       506\n",
      "   macro avg       0.53      0.50      0.50       506\n",
      "weighted avg       0.66      0.67      0.65       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 50, test loss: 0.7984678245815835\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.82      0.85       149\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.60      0.81      0.69       210\n",
      "           3       0.62      0.39      0.48       124\n",
      "\n",
      "    accuracy                           0.67       506\n",
      "   macro avg       0.53      0.50      0.50       506\n",
      "weighted avg       0.66      0.67      0.65       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 51, test loss: 0.7921774641798419\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86       149\n",
      "           1       0.00      0.00      0.00        23\n",
      "           2       0.61      0.82      0.70       210\n",
      "           3       0.62      0.39      0.48       124\n",
      "\n",
      "    accuracy                           0.68       506\n",
      "   macro avg       0.53      0.51      0.51       506\n",
      "weighted avg       0.67      0.68      0.66       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 52, test loss: 0.7859649658203125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.86       149\n",
      "           1       0.10      0.04      0.06        23\n",
      "           2       0.62      0.82      0.71       210\n",
      "           3       0.64      0.41      0.50       124\n",
      "\n",
      "    accuracy                           0.69       506\n",
      "   macro avg       0.56      0.53      0.53       506\n",
      "weighted avg       0.68      0.69      0.67       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 53, test loss: 0.7798534287765563\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.87       149\n",
      "           1       0.18      0.09      0.12        23\n",
      "           2       0.62      0.82      0.71       210\n",
      "           3       0.64      0.43      0.51       124\n",
      "\n",
      "    accuracy                           0.69       506\n",
      "   macro avg       0.59      0.54      0.55       506\n",
      "weighted avg       0.69      0.69      0.68       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 54, test loss: 0.7738345903841403\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.87       149\n",
      "           1       0.18      0.09      0.12        23\n",
      "           2       0.63      0.82      0.71       210\n",
      "           3       0.65      0.44      0.53       124\n",
      "\n",
      "    accuracy                           0.70       506\n",
      "   macro avg       0.59      0.55      0.56       506\n",
      "weighted avg       0.70      0.70      0.69       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 55, test loss: 0.7680451766304348\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.85      0.88       149\n",
      "           1       0.18      0.09      0.12        23\n",
      "           2       0.63      0.82      0.71       210\n",
      "           3       0.65      0.44      0.53       124\n",
      "\n",
      "    accuracy                           0.70       506\n",
      "   macro avg       0.59      0.55      0.56       506\n",
      "weighted avg       0.70      0.70      0.69       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 56, test loss: 0.7625034618754631\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.87       149\n",
      "           1       0.18      0.09      0.12        23\n",
      "           2       0.63      0.81      0.71       210\n",
      "           3       0.64      0.45      0.53       124\n",
      "\n",
      "    accuracy                           0.70       506\n",
      "   macro avg       0.59      0.55      0.56       506\n",
      "weighted avg       0.69      0.70      0.69       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 57, test loss: 0.7571868896484375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.87       149\n",
      "           1       0.18      0.09      0.12        23\n",
      "           2       0.63      0.81      0.71       210\n",
      "           3       0.63      0.45      0.53       124\n",
      "\n",
      "    accuracy                           0.70       506\n",
      "   macro avg       0.59      0.55      0.56       506\n",
      "weighted avg       0.69      0.70      0.68       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 58, test loss: 0.7519984791872529\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.87       149\n",
      "           1       0.20      0.09      0.12        23\n",
      "           2       0.63      0.81      0.71       210\n",
      "           3       0.61      0.46      0.53       124\n",
      "\n",
      "    accuracy                           0.70       506\n",
      "   macro avg       0.59      0.55      0.56       506\n",
      "weighted avg       0.69      0.70      0.68       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 59, test loss: 0.7468104305945837\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.86       149\n",
      "           1       0.22      0.09      0.12        23\n",
      "           2       0.64      0.80      0.71       210\n",
      "           3       0.60      0.47      0.52       124\n",
      "\n",
      "    accuracy                           0.69       506\n",
      "   macro avg       0.59      0.55      0.56       506\n",
      "weighted avg       0.69      0.69      0.68       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 60, test loss: 0.7416055551159523\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.86       149\n",
      "           1       0.25      0.09      0.13        23\n",
      "           2       0.64      0.80      0.71       210\n",
      "           3       0.61      0.49      0.54       124\n",
      "\n",
      "    accuracy                           0.70       506\n",
      "   macro avg       0.60      0.55      0.56       506\n",
      "weighted avg       0.69      0.70      0.69       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 61, test loss: 0.7364039967653779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.86       149\n",
      "           1       0.29      0.09      0.13        23\n",
      "           2       0.65      0.80      0.72       210\n",
      "           3       0.60      0.49      0.54       124\n",
      "\n",
      "    accuracy                           0.70       506\n",
      "   macro avg       0.61      0.55      0.56       506\n",
      "weighted avg       0.70      0.70      0.69       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 62, test loss: 0.7311710161653903\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.87       149\n",
      "           1       0.29      0.09      0.13        23\n",
      "           2       0.65      0.80      0.72       210\n",
      "           3       0.61      0.50      0.55       124\n",
      "\n",
      "    accuracy                           0.71       506\n",
      "   macro avg       0.61      0.56      0.57       506\n",
      "weighted avg       0.70      0.71      0.69       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 63, test loss: 0.7258975062916873\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.84      0.87       149\n",
      "           1       0.29      0.09      0.13        23\n",
      "           2       0.65      0.80      0.72       210\n",
      "           3       0.60      0.50      0.55       124\n",
      "\n",
      "    accuracy                           0.71       506\n",
      "   macro avg       0.61      0.56      0.57       506\n",
      "weighted avg       0.70      0.71      0.70       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 64, test loss: 0.720616517802001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.84      0.88       149\n",
      "           1       0.29      0.09      0.13        23\n",
      "           2       0.66      0.81      0.72       210\n",
      "           3       0.61      0.51      0.55       124\n",
      "\n",
      "    accuracy                           0.71       506\n",
      "   macro avg       0.62      0.56      0.57       506\n",
      "weighted avg       0.70      0.71      0.70       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 65, test loss: 0.7153233464056324\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.84      0.88       149\n",
      "           1       0.29      0.09      0.13        23\n",
      "           2       0.66      0.81      0.73       210\n",
      "           3       0.62      0.53      0.57       124\n",
      "\n",
      "    accuracy                           0.72       506\n",
      "   macro avg       0.62      0.57      0.58       506\n",
      "weighted avg       0.71      0.72      0.71       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 66, test loss: 0.7099629277768342\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.83      0.88       149\n",
      "           1       0.38      0.13      0.19        23\n",
      "           2       0.66      0.81      0.73       210\n",
      "           3       0.60      0.52      0.56       124\n",
      "\n",
      "    accuracy                           0.71       506\n",
      "   macro avg       0.64      0.57      0.59       506\n",
      "weighted avg       0.71      0.71      0.70       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 67, test loss: 0.7046413497020133\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.83      0.88       149\n",
      "           1       0.38      0.13      0.19        23\n",
      "           2       0.66      0.81      0.73       210\n",
      "           3       0.60      0.52      0.55       124\n",
      "\n",
      "    accuracy                           0.71       506\n",
      "   macro avg       0.64      0.57      0.59       506\n",
      "weighted avg       0.71      0.71      0.70       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 68, test loss: 0.6993767659183547\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.83      0.88       149\n",
      "           1       0.43      0.13      0.20        23\n",
      "           2       0.66      0.81      0.73       210\n",
      "           3       0.60      0.52      0.56       124\n",
      "\n",
      "    accuracy                           0.72       506\n",
      "   macro avg       0.65      0.57      0.59       506\n",
      "weighted avg       0.71      0.72      0.71       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 69, test loss: 0.6941447503010746\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.83      0.88       149\n",
      "           1       0.43      0.13      0.20        23\n",
      "           2       0.66      0.81      0.73       210\n",
      "           3       0.61      0.54      0.57       124\n",
      "\n",
      "    accuracy                           0.72       506\n",
      "   macro avg       0.66      0.58      0.60       506\n",
      "weighted avg       0.72      0.72      0.71       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 70, test loss: 0.6890579042698555\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.84      0.88       149\n",
      "           1       0.43      0.13      0.20        23\n",
      "           2       0.67      0.81      0.73       210\n",
      "           3       0.62      0.54      0.58       124\n",
      "\n",
      "    accuracy                           0.72       506\n",
      "   macro avg       0.66      0.58      0.60       506\n",
      "weighted avg       0.72      0.72      0.71       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 71, test loss: 0.6840604397619194\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.84      0.88       149\n",
      "           1       0.43      0.13      0.20        23\n",
      "           2       0.66      0.82      0.73       210\n",
      "           3       0.62      0.53      0.57       124\n",
      "\n",
      "    accuracy                           0.72       506\n",
      "   macro avg       0.66      0.58      0.60       506\n",
      "weighted avg       0.72      0.72      0.71       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 72, test loss: 0.6790197319664032\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.85      0.89       149\n",
      "           1       0.50      0.17      0.26        23\n",
      "           2       0.68      0.82      0.74       210\n",
      "           3       0.64      0.55      0.59       124\n",
      "\n",
      "    accuracy                           0.74       506\n",
      "   macro avg       0.69      0.60      0.62       506\n",
      "weighted avg       0.73      0.74      0.73       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 73, test loss: 0.6738957944123641\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.85      0.89       149\n",
      "           1       0.50      0.17      0.26        23\n",
      "           2       0.68      0.82      0.74       210\n",
      "           3       0.64      0.56      0.59       124\n",
      "\n",
      "    accuracy                           0.74       506\n",
      "   macro avg       0.69      0.60      0.62       506\n",
      "weighted avg       0.73      0.74      0.73       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 74, test loss: 0.6688066565472147\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.85      0.89       149\n",
      "           1       0.60      0.26      0.36        23\n",
      "           2       0.68      0.82      0.74       210\n",
      "           3       0.64      0.55      0.59       124\n",
      "\n",
      "    accuracy                           0.74       506\n",
      "   macro avg       0.71      0.62      0.65       506\n",
      "weighted avg       0.74      0.74      0.73       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 75, test loss: 0.6638726396522975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.85      0.89       149\n",
      "           1       0.67      0.26      0.38        23\n",
      "           2       0.69      0.83      0.75       210\n",
      "           3       0.64      0.55      0.59       124\n",
      "\n",
      "    accuracy                           0.74       506\n",
      "   macro avg       0.73      0.62      0.65       506\n",
      "weighted avg       0.74      0.74      0.73       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 76, test loss: 0.6591374695065464\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90       149\n",
      "           1       0.67      0.26      0.38        23\n",
      "           2       0.69      0.83      0.76       210\n",
      "           3       0.65      0.55      0.59       124\n",
      "\n",
      "    accuracy                           0.75       506\n",
      "   macro avg       0.73      0.63      0.66       506\n",
      "weighted avg       0.75      0.75      0.74       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 77, test loss: 0.6546264769060338\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90       149\n",
      "           1       0.67      0.26      0.38        23\n",
      "           2       0.69      0.83      0.76       210\n",
      "           3       0.65      0.55      0.59       124\n",
      "\n",
      "    accuracy                           0.75       506\n",
      "   macro avg       0.73      0.63      0.66       506\n",
      "weighted avg       0.75      0.75      0.74       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 78, test loss: 0.6502792298087018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90       149\n",
      "           1       0.67      0.26      0.38        23\n",
      "           2       0.69      0.83      0.76       210\n",
      "           3       0.65      0.55      0.59       124\n",
      "\n",
      "    accuracy                           0.75       506\n",
      "   macro avg       0.73      0.63      0.66       506\n",
      "weighted avg       0.75      0.75      0.74       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 79, test loss: 0.6461192496680459\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.87      0.90       149\n",
      "           1       0.70      0.30      0.42        23\n",
      "           2       0.70      0.83      0.76       210\n",
      "           3       0.65      0.55      0.59       124\n",
      "\n",
      "    accuracy                           0.75       506\n",
      "   macro avg       0.74      0.64      0.67       506\n",
      "weighted avg       0.75      0.75      0.74       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 80, test loss: 0.6421429177989131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.87      0.90       149\n",
      "           1       0.70      0.30      0.42        23\n",
      "           2       0.70      0.83      0.76       210\n",
      "           3       0.65      0.55      0.59       124\n",
      "\n",
      "    accuracy                           0.75       506\n",
      "   macro avg       0.74      0.64      0.67       506\n",
      "weighted avg       0.75      0.75      0.74       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 81, test loss: 0.6382782826781744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.87      0.90       149\n",
      "           1       0.73      0.35      0.47        23\n",
      "           2       0.70      0.83      0.76       210\n",
      "           3       0.65      0.55      0.60       124\n",
      "\n",
      "    accuracy                           0.75       506\n",
      "   macro avg       0.75      0.65      0.68       506\n",
      "weighted avg       0.76      0.75      0.75       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 82, test loss: 0.6345043559319417\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90       149\n",
      "           1       0.73      0.35      0.47        23\n",
      "           2       0.70      0.83      0.76       210\n",
      "           3       0.66      0.55      0.60       124\n",
      "\n",
      "    accuracy                           0.75       506\n",
      "   macro avg       0.75      0.65      0.68       506\n",
      "weighted avg       0.76      0.75      0.75       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 83, test loss: 0.6308314508129015\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90       149\n",
      "           1       0.73      0.35      0.47        23\n",
      "           2       0.71      0.83      0.76       210\n",
      "           3       0.66      0.56      0.61       124\n",
      "\n",
      "    accuracy                           0.76       506\n",
      "   macro avg       0.75      0.66      0.69       506\n",
      "weighted avg       0.76      0.76      0.75       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 84, test loss: 0.6272128862825778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90       149\n",
      "           1       0.73      0.35      0.47        23\n",
      "           2       0.71      0.83      0.76       210\n",
      "           3       0.66      0.56      0.61       124\n",
      "\n",
      "    accuracy                           0.76       506\n",
      "   macro avg       0.75      0.66      0.69       506\n",
      "weighted avg       0.76      0.76      0.75       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 85, test loss: 0.6235864209563364\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90       149\n",
      "           1       0.71      0.43      0.54        23\n",
      "           2       0.71      0.83      0.77       210\n",
      "           3       0.66      0.56      0.61       124\n",
      "\n",
      "    accuracy                           0.76       506\n",
      "   macro avg       0.75      0.68      0.70       506\n",
      "weighted avg       0.76      0.76      0.76       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 86, test loss: 0.6199879401286129\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90       149\n",
      "           1       0.73      0.48      0.58        23\n",
      "           2       0.72      0.83      0.77       210\n",
      "           3       0.66      0.56      0.61       124\n",
      "\n",
      "    accuracy                           0.76       506\n",
      "   macro avg       0.76      0.69      0.71       506\n",
      "weighted avg       0.77      0.76      0.76       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 87, test loss: 0.6164605061527297\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91       149\n",
      "           1       0.73      0.48      0.58        23\n",
      "           2       0.72      0.84      0.78       210\n",
      "           3       0.67      0.56      0.61       124\n",
      "\n",
      "    accuracy                           0.77       506\n",
      "   macro avg       0.76      0.69      0.72       506\n",
      "weighted avg       0.77      0.77      0.77       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 88, test loss: 0.6130302941846282\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91       149\n",
      "           1       0.79      0.48      0.59        23\n",
      "           2       0.72      0.84      0.78       210\n",
      "           3       0.67      0.56      0.61       124\n",
      "\n",
      "    accuracy                           0.77       506\n",
      "   macro avg       0.78      0.69      0.72       506\n",
      "weighted avg       0.77      0.77      0.77       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 89, test loss: 0.609690368411098\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91       149\n",
      "           1       0.79      0.48      0.59        23\n",
      "           2       0.72      0.84      0.78       210\n",
      "           3       0.68      0.57      0.62       124\n",
      "\n",
      "    accuracy                           0.77       506\n",
      "   macro avg       0.78      0.69      0.72       506\n",
      "weighted avg       0.77      0.77      0.77       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 90, test loss: 0.6064115984166564\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91       149\n",
      "           1       0.79      0.48      0.59        23\n",
      "           2       0.72      0.84      0.78       210\n",
      "           3       0.68      0.57      0.62       124\n",
      "\n",
      "    accuracy                           0.77       506\n",
      "   macro avg       0.78      0.69      0.72       506\n",
      "weighted avg       0.77      0.77      0.77       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 91, test loss: 0.6031697993221962\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91       149\n",
      "           1       0.86      0.52      0.65        23\n",
      "           2       0.73      0.84      0.78       210\n",
      "           3       0.68      0.58      0.63       124\n",
      "\n",
      "    accuracy                           0.78       506\n",
      "   macro avg       0.80      0.71      0.74       506\n",
      "weighted avg       0.78      0.78      0.77       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 92, test loss: 0.599996212442873\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91       149\n",
      "           1       0.88      0.61      0.72        23\n",
      "           2       0.73      0.84      0.78       210\n",
      "           3       0.68      0.58      0.63       124\n",
      "\n",
      "    accuracy                           0.78       506\n",
      "   macro avg       0.80      0.73      0.76       506\n",
      "weighted avg       0.78      0.78      0.78       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 93, test loss: 0.5968910187129447\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90       149\n",
      "           1       0.88      0.65      0.75        23\n",
      "           2       0.73      0.84      0.78       210\n",
      "           3       0.69      0.58      0.63       124\n",
      "\n",
      "    accuracy                           0.78       506\n",
      "   macro avg       0.81      0.74      0.77       506\n",
      "weighted avg       0.78      0.78      0.78       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 94, test loss: 0.5937916751906805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90       149\n",
      "           1       0.88      0.65      0.75        23\n",
      "           2       0.73      0.84      0.78       210\n",
      "           3       0.69      0.59      0.63       124\n",
      "\n",
      "    accuracy                           0.78       506\n",
      "   macro avg       0.81      0.74      0.77       506\n",
      "weighted avg       0.79      0.78      0.78       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 95, test loss: 0.5906817771700531\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90       149\n",
      "           1       0.88      0.65      0.75        23\n",
      "           2       0.73      0.84      0.78       210\n",
      "           3       0.68      0.59      0.63       124\n",
      "\n",
      "    accuracy                           0.78       506\n",
      "   macro avg       0.81      0.74      0.77       506\n",
      "weighted avg       0.78      0.78      0.78       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 96, test loss: 0.5875874394955842\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90       149\n",
      "           1       0.88      0.65      0.75        23\n",
      "           2       0.73      0.83      0.78       210\n",
      "           3       0.68      0.59      0.63       124\n",
      "\n",
      "    accuracy                           0.78       506\n",
      "   macro avg       0.80      0.74      0.77       506\n",
      "weighted avg       0.78      0.78      0.78       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 97, test loss: 0.5845058275305707\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       149\n",
      "           1       0.89      0.74      0.81        23\n",
      "           2       0.73      0.83      0.78       210\n",
      "           3       0.68      0.59      0.63       124\n",
      "\n",
      "    accuracy                           0.78       506\n",
      "   macro avg       0.81      0.76      0.78       506\n",
      "weighted avg       0.79      0.78      0.78       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 98, test loss: 0.5814189081606658\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       149\n",
      "           1       0.90      0.78      0.84        23\n",
      "           2       0.74      0.83      0.78       210\n",
      "           3       0.68      0.59      0.63       124\n",
      "\n",
      "    accuracy                           0.78       506\n",
      "   macro avg       0.81      0.77      0.79       506\n",
      "weighted avg       0.79      0.78      0.78       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 99, test loss: 0.5783740258499568\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       149\n",
      "           1       0.95      0.83      0.88        23\n",
      "           2       0.74      0.84      0.79       210\n",
      "           3       0.68      0.59      0.63       124\n",
      "\n",
      "    accuracy                           0.79       506\n",
      "   macro avg       0.83      0.78      0.80       506\n",
      "weighted avg       0.79      0.79      0.79       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 100, test loss: 0.5753994666540576\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       149\n",
      "           1       0.95      0.83      0.88        23\n",
      "           2       0.74      0.84      0.79       210\n",
      "           3       0.68      0.59      0.63       124\n",
      "\n",
      "    accuracy                           0.79       506\n",
      "   macro avg       0.83      0.78      0.80       506\n",
      "weighted avg       0.79      0.79      0.79       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 101, test loss: 0.5725128415073801\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       149\n",
      "           1       0.95      0.83      0.88        23\n",
      "           2       0.74      0.83      0.78       210\n",
      "           3       0.67      0.59      0.63       124\n",
      "\n",
      "    accuracy                           0.79       506\n",
      "   macro avg       0.82      0.78      0.80       506\n",
      "weighted avg       0.79      0.79      0.79       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 102, test loss: 0.5697408683686388\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       149\n",
      "           1       0.95      0.83      0.88        23\n",
      "           2       0.74      0.83      0.78       210\n",
      "           3       0.67      0.59      0.63       124\n",
      "\n",
      "    accuracy                           0.79       506\n",
      "   macro avg       0.82      0.78      0.80       506\n",
      "weighted avg       0.79      0.79      0.79       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 103, test loss: 0.5670921717707819\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       149\n",
      "           1       0.95      0.83      0.88        23\n",
      "           2       0.74      0.83      0.78       210\n",
      "           3       0.67      0.59      0.63       124\n",
      "\n",
      "    accuracy                           0.79       506\n",
      "   macro avg       0.82      0.78      0.80       506\n",
      "weighted avg       0.79      0.79      0.79       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 104, test loss: 0.5645645201912982\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       149\n",
      "           1       0.95      0.83      0.88        23\n",
      "           2       0.74      0.83      0.78       210\n",
      "           3       0.67      0.59      0.63       124\n",
      "\n",
      "    accuracy                           0.79       506\n",
      "   macro avg       0.82      0.78      0.80       506\n",
      "weighted avg       0.79      0.79      0.79       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 105, test loss: 0.5621229330070405\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       149\n",
      "           1       0.95      0.87      0.91        23\n",
      "           2       0.74      0.83      0.78       210\n",
      "           3       0.67      0.59      0.63       124\n",
      "\n",
      "    accuracy                           0.79       506\n",
      "   macro avg       0.82      0.79      0.81       506\n",
      "weighted avg       0.79      0.79      0.79       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 106, test loss: 0.5597695211176815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       149\n",
      "           1       0.95      0.87      0.91        23\n",
      "           2       0.74      0.83      0.78       210\n",
      "           3       0.67      0.59      0.63       124\n",
      "\n",
      "    accuracy                           0.79       506\n",
      "   macro avg       0.82      0.79      0.81       506\n",
      "weighted avg       0.79      0.79      0.79       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 107, test loss: 0.557494272827631\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       149\n",
      "           1       0.95      0.87      0.91        23\n",
      "           2       0.74      0.83      0.79       210\n",
      "           3       0.67      0.60      0.63       124\n",
      "\n",
      "    accuracy                           0.79       506\n",
      "   macro avg       0.83      0.79      0.81       506\n",
      "weighted avg       0.79      0.79      0.79       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 108, test loss: 0.5552777678598999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       149\n",
      "           1       0.95      0.87      0.91        23\n",
      "           2       0.74      0.83      0.79       210\n",
      "           3       0.67      0.60      0.63       124\n",
      "\n",
      "    accuracy                           0.79       506\n",
      "   macro avg       0.83      0.79      0.81       506\n",
      "weighted avg       0.79      0.79      0.79       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 109, test loss: 0.5531034808856226\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       149\n",
      "           1       0.95      0.91      0.93        23\n",
      "           2       0.75      0.83      0.79       210\n",
      "           3       0.67      0.60      0.63       124\n",
      "\n",
      "    accuracy                           0.79       506\n",
      "   macro avg       0.83      0.81      0.82       506\n",
      "weighted avg       0.79      0.79      0.79       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 110, test loss: 0.5509743671643405\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       149\n",
      "           1       0.95      0.91      0.93        23\n",
      "           2       0.75      0.83      0.79       210\n",
      "           3       0.67      0.60      0.63       124\n",
      "\n",
      "    accuracy                           0.79       506\n",
      "   macro avg       0.83      0.81      0.82       506\n",
      "weighted avg       0.79      0.79      0.79       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 111, test loss: 0.5488722126474493\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       149\n",
      "           1       0.95      0.91      0.93        23\n",
      "           2       0.75      0.83      0.79       210\n",
      "           3       0.67      0.60      0.63       124\n",
      "\n",
      "    accuracy                           0.79       506\n",
      "   macro avg       0.83      0.81      0.82       506\n",
      "weighted avg       0.79      0.79      0.79       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 112, test loss: 0.5467820601029829\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       149\n",
      "           1       0.95      0.91      0.93        23\n",
      "           2       0.75      0.83      0.79       210\n",
      "           3       0.67      0.60      0.63       124\n",
      "\n",
      "    accuracy                           0.79       506\n",
      "   macro avg       0.83      0.81      0.82       506\n",
      "weighted avg       0.79      0.79      0.79       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 113, test loss: 0.5446761662781003\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       149\n",
      "           1       0.95      0.91      0.93        23\n",
      "           2       0.75      0.83      0.79       210\n",
      "           3       0.67      0.60      0.64       124\n",
      "\n",
      "    accuracy                           0.79       506\n",
      "   macro avg       0.83      0.81      0.82       506\n",
      "weighted avg       0.80      0.79      0.79       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 114, test loss: 0.5425706343217329\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       149\n",
      "           1       0.95      0.91      0.93        23\n",
      "           2       0.75      0.83      0.79       210\n",
      "           3       0.67      0.60      0.64       124\n",
      "\n",
      "    accuracy                           0.79       506\n",
      "   macro avg       0.83      0.81      0.82       506\n",
      "weighted avg       0.80      0.79      0.79       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 115, test loss: 0.5404874175904768\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       149\n",
      "           1       0.95      0.91      0.93        23\n",
      "           2       0.75      0.83      0.79       210\n",
      "           3       0.67      0.60      0.64       124\n",
      "\n",
      "    accuracy                           0.79       506\n",
      "   macro avg       0.83      0.81      0.82       506\n",
      "weighted avg       0.80      0.79      0.79       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 116, test loss: 0.5384409908249445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       149\n",
      "           1       0.95      0.91      0.93        23\n",
      "           2       0.75      0.83      0.79       210\n",
      "           3       0.67      0.60      0.64       124\n",
      "\n",
      "    accuracy                           0.79       506\n",
      "   macro avg       0.83      0.81      0.82       506\n",
      "weighted avg       0.80      0.79      0.79       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 117, test loss: 0.5364587354094614\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       149\n",
      "           1       0.95      0.91      0.93        23\n",
      "           2       0.75      0.83      0.79       210\n",
      "           3       0.67      0.61      0.64       124\n",
      "\n",
      "    accuracy                           0.80       506\n",
      "   macro avg       0.83      0.81      0.82       506\n",
      "weighted avg       0.80      0.80      0.80       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 118, test loss: 0.5345389626242898\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       149\n",
      "           1       0.95      0.91      0.93        23\n",
      "           2       0.75      0.83      0.79       210\n",
      "           3       0.67      0.61      0.64       124\n",
      "\n",
      "    accuracy                           0.80       506\n",
      "   macro avg       0.83      0.81      0.82       506\n",
      "weighted avg       0.80      0.80      0.80       506\n",
      "\n",
      "[0 1 2 3]\n",
      "Epoch: 119, test loss: 0.5326642424692749\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91       149\n",
      "           1       0.95      0.91      0.93        23\n",
      "           2       0.76      0.83      0.80       210\n",
      "           3       0.68      0.63      0.65       124\n",
      "\n",
      "    accuracy                           0.80       506\n",
      "   macro avg       0.83      0.81      0.82       506\n",
      "weighted avg       0.80      0.80      0.80       506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 120\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "batch_size = 64\n",
    "num_classes = 4\n",
    "lr = 1e-4\n",
    "\n",
    "import torchvision\n",
    "model = Net()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "for epoch in range(epochs):\n",
    "    train(model, X_train, y_train, optimizer)\n",
    "    test_loss, cr = test(model, X_test, y_test)\n",
    "    print(\"Epoch: {}, test loss: {}\".format(epoch, test_loss))\n",
    "    print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ey6jmt83MHw1",
    "outputId": "36088fa7-be66-492c-d62e-33bfbc3129af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function save in module torch.serialization:\n",
      "\n",
      "save(obj, f: Union[str, os.PathLike, BinaryIO, IO[bytes]], pickle_module=<module 'pickle' from '/usr/lib/python3.7/pickle.py'>, pickle_protocol=2, _use_new_zipfile_serialization=True) -> None\n",
      "    Saves an object to a disk file.\n",
      "    \n",
      "    See also: `saving-loading-tensors`\n",
      "    \n",
      "    Args:\n",
      "        obj: saved object\n",
      "        f: a file-like object (has to implement write and flush) or a string or\n",
      "           os.PathLike object containing a file name\n",
      "        pickle_module: module used for pickling metadata and objects\n",
      "        pickle_protocol: can be specified to override the default protocol\n",
      "    \n",
      "    .. note::\n",
      "        A common PyTorch convention is to save tensors using .pt file extension.\n",
      "    \n",
      "    .. note::\n",
      "        PyTorch preserves storage sharing across serialization. See\n",
      "        `preserve-storage-sharing` for more details.\n",
      "    \n",
      "    .. note::\n",
      "        The 1.6 release of PyTorch switched ``torch.save`` to use a new\n",
      "        zipfile-based file format. ``torch.load`` still retains the ability to\n",
      "        load files in the old format. If for any reason you want ``torch.save``\n",
      "        to use the old format, pass the kwarg ``_use_new_zipfile_serialization=False``.\n",
      "    \n",
      "    Example:\n",
      "        >>> # Save to file\n",
      "        >>> x = torch.tensor([0, 1, 2, 3, 4])\n",
      "        >>> torch.save(x, 'tensor.pt')\n",
      "        >>> # Save to io.BytesIO buffer\n",
      "        >>> buffer = io.BytesIO()\n",
      "        >>> torch.save(x, buffer)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "mOv6xHUdTvpu"
   },
   "outputs": [],
   "source": [
    "with open('signal_classifier.torch', 'wb') as f:\n",
    "    torch.save(model, f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "cut_traffic_signals_dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
